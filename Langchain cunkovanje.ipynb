{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c516772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32319aae",
   "metadata": {},
   "source": [
    "os.environ['OPENAI_API_KEY'] = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f813570",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767bf94",
   "metadata": {},
   "source": [
    "Prolazimo kroz PDF fajl i sa svek stranice izvlacimo tekst. Varijabla tekst na kraju sadrzi sav tekst sa slike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c998b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "file_path = 'C:/Users/enaf/Documents/OPENAI/Različite tehnike čunkovanja dokumenata.pdf'\n",
    "text = ''\n",
    "# Otvori PDF fajl\n",
    "with open(file_path, 'rb') as pdf_file:\n",
    "    pdf_reader =PdfReader(pdf_file)\n",
    "    for page in pdf_reader.pages:\n",
    "            text += page.extract_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975f1cf",
   "metadata": {},
   "source": [
    "To count the number of tokens that gpt-3.5-turbo will use for some text we need to initialize the tiktoken tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab5024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddfbbd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2529"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts = tiktoken_len(text)\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcbe235",
   "metadata": {},
   "source": [
    "Nas dokument ima ukupno 2529 tokena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "258fc813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nRazličite tehnike čunkovanja dokumenata  \\n \\nŠta su tokeni ? \\n \\nTokeni  se mogu definisati kao delovi re či. Pre nego što API procesuira prompt, ulaz se razlaže \\nna tokene. Ti tokeni su osnovna merna jedinica kojom se računaju troškovi korišćenja \\nodređenog OpenAI modela, o čemu će kasnije biti više reči.  \\nKako biste stekli bolju predstavu o veličini tokena, naredna pravila objašnjavaju odnos između \\ntokena i karaktera/reči u engleskom jeziku : \\n• 1 token  ~= 4 karaktera  \\n• 1 token ~= ¾ reči  \\n• 100 t okena ~= 75 reči  \\nSplitovanje teksta u tokene zavisi od jezika. Npr. Cómo estás’ na Španskom sadrži  5 tokena  (za \\n10 karaktera ). Zbog toga je implementacija API -a skuplja ako se ne radi o engleskom jeziku.  \\nRačunanje tokena na konkretnim primerima možete istr ažiti na linku: \\nhttps://platform.openai.com/tokenizer  \\nOgraničenja tokena  \\n \\nTokene treba poznavati, jer različiti modeli imaju različita ograničenja u pogledu maksimalne \\ndozvoljene veličine zahteva. Maksimalni broj tokena za gpt -4 je 8192. M aksimalna veličina \\nzahteva za gpt-3.5-turbo   iznosi 4097 tokena, što znači da ulazni prompt i odgovor koji \\ndaje OpenAI dele 4097 tokena.  Ovaj problem postaje značaj ukoliko se prosleđuje i istorija \\nrazgovora , jer se kroz razgovor količina ulaza u model povećava, jer se svaki put šalje \\nkumulativan tog razgovora.  \\nMaksimalan broj tokena po za htevu za svaki model možete pronaći na sledećem linku : \\nhttps://platform.openai.com/docs/models/  \\nCene za 1000 tokena mozete pronaći na: https://openai.com/pricing  \\nZa pomoć prilikom računanja cene možete posetiti razne sajtove koje računaju cene za \\nOpenAI modele: https://gptforwork.com/tools/openai -chatgpt -api-pricing -calculator  \\n \\nOptmizacija prompt -a \\n \\nKako bi prompt bio što kvalitetniji, i sadržao relevantne podatke, razvijene su različite tehnike \\nsplitovanja (čankovanja) teksta koj im se prevazilaze ograničenja u pogledu broja tokena.   \\nspaCy biblioteka  \\n \\nspaCy  je popularna open -source Python biblioteka koja se koristi za obradu prirodnog jezika \\n(NLP - Natural Language Processing ). Ova biblioteka je razvijena kako bi omogućila brzu i \\nefikasnu obradu teksta, uključujući segmentaciju teksta na rečenice i tokene, izdvajanje \\nentiteta (imenovanih entiteta kao što su imena, datumi, lokacije), analizu zavisnosti između \\nreči i još mnogo toga.  \\nspaCy podržava više od  70 jezika. Nažalost, srpski je zik još uvek nije podržan, ali se može \\nkoristiti za hrvatski i slovenački.  \\nGithub projekta gde se koristi spaCy biblioteka : https://github.com/enaxfa/Chat -with -PDF \\n \\n \\n \\n  \\n \\nTokenizacija  \\n \\nU dubokom učenju, tokenizacija  je proces pretvaranja niza karaktera u niz tokena koji se dalje \\nmora pretvoriti u niz numeričkih vektora koji mogu biti obrađeni od strane neuronske mreže.  \\nPostoje različiti načini tokenizacije teksta:  \\n1. Toke nizacija na osnovu reči  \\n2. Tokenizacija na osnovu karaktera  \\n \\nTOKENIZACIJA NA OSNOVU REČI  \\n \\nJednostavan metod kojim ćemo reči iz teksta izvući deljenjem teksta po razmacima.   \\n \\nPrilično jednostavan način tokenizacije, ali koji sa sobom nosi određene mane.  \\nNEDOSTACI TOKENIZACIJE NA OSNOVU REČI  \\n \\n• Rizik od propuštanja reči u trening podacima: sa tokenizacijom zasnovanim na rečima, \\nvaš model neće prepoznati varijacije reči koje nisu bile deo podataka na kojima je \\nmodel treniran. Dakle, ako je vaš model video ,,stopalo ” i ,,lopta “ u trening podacima, \\nali finalni tekst sadrži ,,fudbal “, model neće moći da prepozna tu reč i biće tretirana sa \\n<UNK> tokenom.  \\n• Rukovanje slengom i skraćenicama: još jedan problem je upotreba slenga i skraćenica \\nu tekstovima ovih dana, ka o što su \" bgm \", \"LOL\", \" dr\" itd. Šta radimo sa ovakvim \\nrečima?  \\n• Šta ako je jezik koji se koristi ne koristi razmake za segmentaciju: za jezik poput \\nkineskog, koji ne koristi razmake za razdvajanje reči, ova vrsta tokenizacije će potpuno \\nzakazati.  \\n  \\n \\nTOKENIZA CIJA ZASNOVANA NA KARAKTERIMA  \\n \\nDa bi se rešili problemi povezani sa tokenizacijom na osnovu reči, isproban je alternativni \\npristup tokenizacije karakter po karakter.  \\nOvo je zaista rešilo problem propuštanja reči, jer sada radimo sa karakterima koji se mogu  \\nenkodirati pomoću ASCII -ja ili Unicode -a i mogu generisati embedding  za bilo koju reč.  \\nSvakom karakteru, bilo da je razmak ili apostrof ili dvotačka, može se dodeliti simbol kako bi \\nse generisao niz vektora.  \\nMeđutim, ovaj pristup je imao svoje mane.  \\nNEDOS TACI TOKENIZACIJ E NA OSNOVU KARAKTER A \\n \\n• Potreba za više računarskih resursa: modeli zasnovani na karakterima će tretirati svaki \\nkarakter kao token, a veći broj tokena znači više ulaznih izračunavanja za obradu \\nsvakog tokena, što zauzvrat zahteva više računa rskih resursa. Za rečenicu sa 5 reči, \\nmože biti potrebno obraditi 30 tokena umesto 5 rečnih tokena.  \\n \\n• Smanjuje broj NLP zadataka i aplikacija: sa dugim nizovima karaktera, moguće je \\nkoristiti samo određenu vrstu neuronskih mrežnih arhitektura. To postavlja \\nograničenje na vrste NLP zadataka koje možemo izvoditi. Za aplikacije poput \\nprepoznavanja entiteta ili klasifikacije teksta, enkodiranje zasnovano na karakterima \\nmože se pokazati kao neefikasan pristup.  \\n \\n \\n \\n \\n \\n  \\n \\nČUNKOVANJE PO REGEXU  \\n \\nChunkovanje pomoću regularnih izraza ( regex ) često se koristi kada želimo da pretražimo ili \\nrazdvojimo tekst na osnovu određenih obrazaca. Evo primera kako možete upotrebiti regex \\nza chunkovanje:  \\n \\n \\nU ovom primeru, koristimo re.findall funkciju koja pretra žuje unapred definisani regex \\nobrazac ( datum_obrazac ) u datom tekstu. Obrazac r\"\\\\d{4} -\\\\d{2} -\\\\d{2}\"  prepoznaje datume \\nu formatu \"YYYY -MM -DD\". Rezultat će biti lista datuma koje je regex pronašao u tekstu.  \\n \\nAko želite da podelite tekst koristeći regex, možet e koristiti re.split  funkciju:  \\n \\n \\nRecimo da imate tekstualne zapise koji sadrže informacije o osobama u formatu \"Ime \\nPrezime (Zanimanje)\" i želite da izdvojite imena, prezimena i zanimanja iz tih zapisa.  \\n \\n \\nMože se zaključiti da za čunkovanje teksta postoje  različite tehnike, ali treba pronaći onu koja \\nće biti najpogodnija za tip teksta koji čunkujemo, i shodno tome prilagođavati različite \\nnavedene metode.  \\n  \\n \\nLANGCHAIN  I TIKTOKEN  \\n \\nTiktoken je biblioteka koja račna broj tokena u nekom tekstu, u zavisnosti od m odela. Različiti \\nLLM modeli koriste različite encoder -e. Spisak modela sa nazivima njihovih encoder -a se \\nmogu naći na : tiktoken/tiktoken/model.py at main · openai/tiktoken · GitHu b \\nModeli gpt-4, gpt-3.5-turbo,  text-embedding -ada-002 koriste cl100k_base  encoder.  \\nMetodom tiktoken. encoding_for_model(\\'gpt -3.5-turbo\\')  vraća se naziv encoder -a za dati \\nmodel, a to je cl100k_base u ovom slučaju . \\nLink do Github repositorijuma na ovu temu se mo že naći na : \\nhttps://github.com/enaxfa/Langchain_TikToken_Chunkin g \\n \\n '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89904198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n', '\\n', '.']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48b07c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ad32e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Različite tehnike čunkovanja dokumenata  \\n \\nŠta su tokeni ? \\n \\nTokeni  se mogu definisati kao delovi re či. Pre nego što API procesuira prompt, ulaz se razlaže \\nna tokene. Ti tokeni su osnovna merna jedinica kojom se računaju troškovi korišćenja \\nodređenog OpenAI modela, o čemu će kasnije biti više reči.  \\nKako biste stekli bolju predstavu o veličini tokena, naredna pravila objašnjavaju odnos između \\ntokena i karaktera/reči u engleskom jeziku : \\n• 1 token  ~= 4 karaktera  \\n• 1 token ~= ¾ reči  \\n• 100 t okena ~= 75 reči  \\nSplitovanje teksta u tokene zavisi od jezika. Npr. Cómo estás’ na Španskom sadrži  5 tokena  (za \\n10 karaktera ). Zbog toga je implementacija API -a skuplja ako se ne radi o engleskom jeziku.  \\nRačunanje tokena na konkretnim primerima možete istr ažiti na linku: \\nhttps://platform.openai.com/tokenizer  \\nOgraničenja tokena  \\n \\nTokene treba poznavati, jer različiti modeli imaju različita ograničenja u pogledu maksimalne'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "277f14a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27f5d7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ograničenja tokena  \\n \\nTokene treba poznavati, jer različiti modeli imaju različita ograničenja u pogledu maksimalne \\ndozvoljene veličine zahteva. Maksimalni broj tokena za gpt -4 je 8192. M aksimalna veličina \\nzahteva za gpt-3.5-turbo   iznosi 4097 tokena, što znači da ulazni prompt i odgovor koji \\ndaje OpenAI dele 4097 tokena.  Ovaj problem postaje značaj ukoliko se prosleđuje i istorija \\nrazgovora , jer se kroz razgovor količina ulaza u model povećava, jer se svaki put šalje \\nkumulativan tog razgovora.  \\nMaksimalan broj tokena po za htevu za svaki model možete pronaći na sledećem linku : \\nhttps://platform.openai.com/docs/models/  \\nCene za 1000 tokena mozete pronaći na: https://openai.com/pricing  \\nZa pomoć prilikom računanja cene možete posetiti razne sajtove koje računaju cene za \\nOpenAI modele: https://gptforwork.com/tools/openai -chatgpt -api-pricing -calculator  \\n \\nOptmizacija prompt -a \\n \\nKako bi prompt bio što kvalitetniji, i sadržao relevantne podatke, razvijene su različite tehnike'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bea9caaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50545742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Optmizacija prompt -a \\n \\nKako bi prompt bio što kvalitetniji, i sadržao relevantne podatke, razvijene su različite tehnike \\nsplitovanja (čankovanja) teksta koj im se prevazilaze ograničenja u pogledu broja tokena.   \\nspaCy biblioteka  \\n \\nspaCy  je popularna open -source Python biblioteka koja se koristi za obradu prirodnog jezika \\n(NLP - Natural Language Processing ). Ova biblioteka je razvijena kako bi omogućila brzu i \\nefikasnu obradu teksta, uključujući segmentaciju teksta na rečenice i tokene, izdvajanje \\nentiteta (imenovanih entiteta kao što su imena, datumi, lokacije), analizu zavisnosti između \\nreči i još mnogo toga.  \\nspaCy podržava više od  70 jezika. Nažalost, srpski je zik još uvek nije podržan, ali se može \\nkoristiti za hrvatski i slovenački.  \\nGithub projekta gde se koristi spaCy biblioteka : https://github.com/enaxfa/Chat -with -PDF \\n \\n \\n \\n  \\n \\nTokenizacija  \\n \\nU dubokom učenju, tokenizacija  je proces pretvaranja niza karaktera u niz tokena koji se dalje'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43622de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len(chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf151a1",
   "metadata": {},
   "source": [
    "Iako se nekada desi da se prekine chunk u pola recenice i da se izgubi znacenje, sa overlap \n",
    "parametrom omogucavamo da pocetak narednog chunka bude 50 tokena iza kraja i da se uhvati semantika isecene recenice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a5c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
